{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from datasets import load_from_disk\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/logs/jtorresb/Geneformer/yeast/pretraining/models/250225_192022_yeastformer_L4_emb256_SL512_E20_B8_LR0.0016_LScosine_WU50_Oadamw_torch/models\"\n",
    "token_dict_path = \"/home/logs/jtorresb/Geneformer/yeast/yeast_data/output/yeast_token_dict.pkl\"\n",
    "\n",
    "# Load gene token dictionary\n",
    "with open(token_dict_path, \"rb\") as fp:\n",
    "    token_dictionary = pickle.load(fp)\n",
    "\n",
    "# Load model\n",
    "model = BertForMaskedLM.from_pretrained(model_path)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(device)\n",
    "\n",
    "# Invert the token dictionary (if it maps gene_name -> token_id)\n",
    "id_to_gene = {v: k for k, v in token_dictionary.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Load dataset\n",
    "# -------------------------------\n",
    "dataset = load_from_disk(\"/home/logs/jtorresb/Geneformer/yeast/yeast_data/output/yeast_master_matrix_sgd.dataset\")\n",
    "# In this example, we use the full dataset (no train-test split)\n",
    "\n",
    "# -------------------------------\n",
    "# Define a collate function for batching\n",
    "# -------------------------------\n",
    "def collate_fn(samples):\n",
    "    # Each sample is expected to be a dictionary with key \"input_ids\"\n",
    "    # Here we assume all input_ids are already padded to the same length.\n",
    "    input_ids = torch.tensor([s[\"input_ids\"] for s in samples])\n",
    "    return {\"input_ids\": input_ids}\n",
    "\n",
    "# Create a DataLoader with an appropriate batch size\n",
    "batch_size = 8  # Adjust based on your GPU memory\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Aggregation configuration\n",
    "# -------------------------------\n",
    "# Define weights for ranking: rank 1 gets 5 points, then 4, 3, 2, and 1.\n",
    "rank_weights = [5, 4, 3, 2, 1]\n",
    "\n",
    "# Dictionary to store aggregated attention scores.\n",
    "# Keys: source gene names (the gene paying attention)\n",
    "# Values: dictionaries mapping target gene names -> aggregated weighted score.\n",
    "gene_attention_scores = {}\n",
    "\n",
    "# -------------------------------\n",
    "# Process each batch from the DataLoader\n",
    "# -------------------------------\n",
    "for batch in tqdm(dataloader, desc=\"Processing batches\"):\n",
    "    # Move input_ids to device; shape: [batch_size, seq_len]\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "\n",
    "    # Forward pass with output_attentions=True to obtain attention matrices.\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, output_attentions=True)\n",
    "    \n",
    "    # Extract attentions from the last layer.\n",
    "    # outputs.attentions[-1] has shape: [batch_size, num_heads, seq_len, seq_len]\n",
    "    last_layer_attentions = outputs.attentions[-1]\n",
    "    b_size, num_heads, seq_len, _ = last_layer_attentions.shape\n",
    "\n",
    "    # Iterate over each sample in the batch\n",
    "    for b in range(b_size):\n",
    "        # Get input_ids for this sample as a list (to index the token IDs)\n",
    "        sample_input_ids = input_ids[b].cpu().tolist()\n",
    "        # Get attention matrices for this sample; shape: [num_heads, seq_len, seq_len]\n",
    "        sample_attentions = last_layer_attentions[b]\n",
    "\n",
    "        # For each head in the sample:\n",
    "        for head in range(num_heads):\n",
    "            att_matrix = sample_attentions[head]  # shape: [seq_len, seq_len]\n",
    "            # For every token position in the sentence (each representing a source gene)\n",
    "            for i in range(seq_len):\n",
    "                source_token_id = sample_input_ids[i]\n",
    "                # Skip if token id is not in the id_to_gene mapping.\n",
    "                if source_token_id not in id_to_gene:\n",
    "                    continue\n",
    "                source_gene = id_to_gene[source_token_id]\n",
    "                # Initialize the dictionary for this source gene if not already present.\n",
    "                if source_gene not in gene_attention_scores:\n",
    "                    gene_attention_scores[source_gene] = {}\n",
    "\n",
    "                # Get the attention vector for the source gene (row i in the attention matrix).\n",
    "                # This vector indicates how much attention the source gene pays to every token.\n",
    "                att_vector = att_matrix[i].clone()\n",
    "                # Exclude self-attention by setting the score for position i to -infinity.\n",
    "                att_vector[i] = float('-inf')\n",
    "                # Obtain the top 5 indices (tokens) with the highest attention scores.\n",
    "                topk = torch.topk(att_vector, k=5)\n",
    "                top_indices = topk.indices  # indices of tokens in the top-5\n",
    "\n",
    "                # For each rank (0 = highest, 4 = lowest), assign a weighted score.\n",
    "                for rank, target_position in enumerate(top_indices):\n",
    "                    # Convert target_position (tensor) to an integer index\n",
    "                    target_index = target_position.item()\n",
    "                    target_token_id = sample_input_ids[target_index]\n",
    "                    if target_token_id not in id_to_gene:\n",
    "                        continue\n",
    "                    target_gene = id_to_gene[target_token_id]\n",
    "                    weight = rank_weights[rank]\n",
    "                    # Update the aggregated score for (source_gene -> target_gene)\n",
    "                    gene_attention_scores[source_gene][target_gene] = (\n",
    "                        gene_attention_scores[source_gene].get(target_gene, 0) + weight\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Save top 5 most important genes for each gene to a text file\n",
    "# -------------------------------\n",
    "output_file = \"top5_genes.txt\"\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    for source_gene, targets in gene_attention_scores.items():\n",
    "        # Sort target genes by aggregated score (highest first)\n",
    "        sorted_targets = sorted(targets.items(), key=lambda x: x[1], reverse=True)\n",
    "        # Select the top 5 targets\n",
    "        top5 = sorted_targets[:5]\n",
    "        # Write the source gene and its top 5 targets to the file\n",
    "        f.write(f\"{source_gene}:\\n\")\n",
    "        for rank, (target_gene, score) in enumerate(top5, start=1):\n",
    "            f.write(f\"    {rank}. {target_gene} (score: {score})\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"Top 5 genes for each gene have been written to '{output_file}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geneformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
