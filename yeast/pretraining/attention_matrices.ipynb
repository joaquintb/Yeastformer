{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== STEP 1: Load Pretrained Model & Token Dictionary ==========\n",
    "model_path = \"/home/logs/jtorresb/Geneformer/yeast/pretraining/models/250218_185527_yeastformer_L4_emb256_SL512_E10_B8_LR0.0016_LScosine_WU53_Oadamw_torch/checkpoint-2000\"\n",
    "token_dict_path = \"/home/logs/jtorresb/Geneformer/yeast/yeast_data/output/yeast_token_dict.pkl\"\n",
    "\n",
    "# Load gene token dictionary\n",
    "with open(token_dict_path, \"rb\") as fp:\n",
    "    token_dictionary = pickle.load(fp)\n",
    "\n",
    "# Load model\n",
    "model = BertForMaskedLM.from_pretrained(model_path)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== STEP 2: Load a Sample Gene Sequence ==========\n",
    "# Let's assume you have a dataset of 11,889 sequences of length 512\n",
    "dataset_path = \"/home/logs/jtorresb/Geneformer/yeast/yeast_data/output/yeast_master_matrix_sgd.dataset\"\n",
    "dataset = load_from_disk(dataset_path)\n",
    "\n",
    "# Select a random sequence from dataset (shape: [512 tokens])\n",
    "sample_idx = 150  # Change this to test different samples\n",
    "gene_sequence = dataset[sample_idx][\"input_ids\"]  # Assuming it's stored as \"input_ids\"\n",
    "\n",
    "# Convert to tensor & add batch dimension\n",
    "input_ids = torch.tensor([gene_sequence]).to(device)  # Shape: [1, 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== STEP 3: Get Attention Matrices ==========\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, output_attentions=True)\n",
    "    attentions = outputs.attentions  # List of tensors (one per layer)\n",
    "\n",
    "# Convert to numpy for easier analysis\n",
    "num_layers = len(attentions)\n",
    "num_heads = attentions[0].shape[1]  # Attention heads\n",
    "\n",
    "# Print model details\n",
    "print(f\"Extracted attention from {num_layers} layers and {num_heads} attention heads.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== STEP 4: Visualize Attention (Optional) ==========\n",
    "# Pick a layer and head to visualize\n",
    "layer_idx = 0  # Change to inspect different layers\n",
    "head_idx = 0   # Change to inspect different heads\n",
    "\n",
    "# Extract attention matrix for the selected layer and head\n",
    "attn_matrix = attentions[layer_idx].squeeze(0)[head_idx].cpu().numpy()  # Shape: [512, 512]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geneformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
