{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulding Geneformer-Like Dataset From Yeast Master Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building .loom file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Replacing NaNs by 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"/home/logs/jtorresb/Geneformer/yeast/yeast_data/output/original_yeast_master_matrix_sgd_copy.csv\"\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(file_path, sep='\\t', index_col=0) # Important to keep sep='\\t', since that's how it was saved \n",
    "\n",
    "# print(df.shape)\n",
    "\n",
    "# Replace NaNs with 0\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Save the cleaned CSV\n",
    "df.to_csv(file_path, sep='\\t')\n",
    "\n",
    "print(\"NaNs replaced with 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Translating to .loom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import loompy\n",
    "\n",
    "input_file_path = \"/home/logs/jtorresb/Geneformer/yeast/yeast_data/output/original_yeast_master_matrix_sgd_copy.csv\"\n",
    "output_file_path = \"/home/logs/jtorresb/Geneformer/yeast/yeast_data/output/yeast_master_matrix_sgd.loom\"\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(input_file_path, sep='\\t', index_col=0) # Important to keep sep='\\t', since that's how it was saved \n",
    "\n",
    "# Compute total read counts for each experiment (equivalent to cell in geneformer)\n",
    "n_counts = df.sum(axis=0).astype(np.float32)  # Sum across genes for each column \n",
    "\n",
    "# Prepare row attributes (Gene IDs â†’ Ensembl IDs assumed to be index)\n",
    "row_attrs = {\"ensembl_id\": df.index.tolist()}  # Ensure index has Ensembl IDs\n",
    "\n",
    "# Prepare column attributes (Cells & their total read counts)\n",
    "col_attrs = {\n",
    "    # \"exp_name\": df.columns.tolist(),  # Experiment names\n",
    "    \"n_counts\": n_counts.values,   # Total counts per column (experiment)\n",
    "}\n",
    "\n",
    "# Convert DataFrame to Loom format & save\n",
    "loompy.create(output_file_path, df.values.astype(np.float32), row_attrs, col_attrs)\n",
    "\n",
    "print(f\"Loom file saved as: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)  # Should print something like 1.26.4\n",
    "\n",
    "# Had to downgrade numpy < 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Verifying .loom file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loompy\n",
    "\n",
    "input_loom_file_path = \"/home/logs/jtorresb/Geneformer/yeast/yeast_data/output/yeast_master_matrix_sgd.loom\"\n",
    "\n",
    "with loompy.connect(input_loom_file_path) as ds:\n",
    "    # Print general metadata\n",
    "    print(\"Row attributes:\", ds.ra.keys())  # Should contain 'ensembl_id'\n",
    "    print(\"Column attributes:\", ds.ca.keys())  # Should contain 'n_counts'\n",
    "    print(\"Data shape (genes x exp columns):\", ds.shape)\n",
    "\n",
    "    # Print first 5 genes (rows) and their attributes\n",
    "    print(\"\\nFirst 5 Row Attributes:\")\n",
    "    for key in ds.ra.keys():\n",
    "        print(f\"{key}: {ds.ra[key][:5]}\")  # Print first 5 values of each row attribute\n",
    "\n",
    "    # print(f\"exp_name: {ds.ca['exp_name'][:1]}\")\n",
    "    print(f\"n_counts: {ds.ca['n_counts'][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Dictionaries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokens Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Load example to see the intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Path to the token dictionary file\n",
    "token_dict_file = \"/home/logs/jtorresb/Geneformer/geneformer/token_dictionary_gc95M.pkl\"\n",
    "\n",
    "# Function to inspect the token dictionary\n",
    "def inspect_token_dictionary(file_path, num_samples=10):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        token_dict = pickle.load(f)\n",
    "    \n",
    "    print(f\"Token dictionary type: {type(token_dict)}\")\n",
    "    print(f\"Total tokens: {len(token_dict)}\")\n",
    "    print(\"First 10 token entries:\")\n",
    "    sample_items = list(token_dict.items())[:num_samples]\n",
    "    for key, value in sample_items:\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "# Run the inspection\n",
    "inspect_token_dictionary(token_dict_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Generating Token Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# File paths\n",
    "csv_file = \"/home/logs/jtorresb/Geneformer/yeast/yeast_data/output/original_yeast_master_matrix_sgd.csv\"\n",
    "output_pkl = \"/home/logs/jtorresb/Geneformer/yeast/yeast_data/output/yeast_token_dict.pkl\"\n",
    "\n",
    "# Load CSV (Ensure YORFs are the index)\n",
    "df = pd.read_csv(csv_file, sep='\\t', index_col=0)\n",
    "\n",
    "# Extract yeast ORFs (YORFs) from index\n",
    "yorfs = df.index.tolist()\n",
    "\n",
    "# Optional: Sort alphabetically for consistency\n",
    "yorfs.sort()\n",
    "\n",
    "# Initialize token dictionary with special tokens\n",
    "token_dict = {\n",
    "    \"<pad>\": 0,\n",
    "    \"<mask>\": 1,\n",
    "    \"<cls>\": 2,\n",
    "    \"<eos>\": 3,\n",
    "}\n",
    "\n",
    "# Assign unique token IDs starting from 4\n",
    "for i, gene_id in enumerate(yorfs, start=4):\n",
    "    token_dict[gene_id] = i\n",
    "\n",
    "# Save dictionary as a pickle file\n",
    "with open(output_pkl, \"wb\") as f:\n",
    "    pickle.dump(token_dict, f)\n",
    "\n",
    "print(f\"Token dictionary saved as: {output_pkl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Medians Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Inspecting example first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Path to the median dictionary file\n",
    "median_dict_file = \"/home/logs/jtorresb/Geneformer/geneformer/gene_median_dictionary_gc95M.pkl\"\n",
    "\n",
    "# Function to inspect the median dictionary\n",
    "def inspect_median_dictionary(file_path, num_samples=10):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        median_dict = pickle.load(f)\n",
    "    \n",
    "    print(f\"Median dictionary type: {type(median_dict)}\")\n",
    "    print(f\"Total genes in dictionary: {len(median_dict)}\")\n",
    "    print(\"First 10 median entries:\")\n",
    "    sample_items = list(median_dict.items())[:num_samples]\n",
    "    for key, value in sample_items:\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "# Run the inspection\n",
    "inspect_median_dictionary(median_dict_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# File paths\n",
    "csv_file = \"/home/logs/jtorresb/Geneformer/yeast/yeast_data/output/original_yeast_master_matrix_sgd_copy.csv\" # Copy already replaced NaNs by 0s\n",
    "output_pkl = \"/home/logs/jtorresb/Geneformer/yeast/yeast_data/output/yeast_median_dict.pkl\"\n",
    "\n",
    "# Load CSV (genes as index, experiments as columns)\n",
    "df = pd.read_csv(csv_file, sep='\\t', index_col=0)\n",
    "\n",
    "# Compute nonzero medians for each gene\n",
    "median_dict = {}\n",
    "for gene in df.index:\n",
    "    nonzero_values = df.loc[gene][df.loc[gene] != 0]  # Ignore zeros\n",
    "    if not nonzero_values.empty:\n",
    "        median_dict[gene] = np.median(nonzero_values)  # Compute median\n",
    "    else:\n",
    "        median_dict[gene] = 0  # If all values are zero, set median to 0\n",
    "\n",
    "# Save dictionary as a pickle file\n",
    "with open(output_pkl, \"wb\") as f:\n",
    "    pickle.dump(median_dict, f)\n",
    "\n",
    "print(f\"Median dictionary saved as: {output_pkl}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing Loom File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quick model_input_size ChecK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loompy\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# File paths (update these as needed)\n",
    "loom_file_path = \"/home/logs/jtorresb/Geneformer/yeast/yeast_data/output/yeast_master_matrix_sgd.loom\"\n",
    "median_dict_file = \"/home/logs/jtorresb/Geneformer/yeast/yeast_data/output/yeast_median_dict.pkl\"\n",
    "token_dict_file = \"/home/logs/jtorresb/Geneformer/yeast/yeast_data/output/yeast_token_dict.pkl\"\n",
    "\n",
    "# Load the median dictionary and token dictionary\n",
    "with open(median_dict_file, \"rb\") as f:\n",
    "    median_dict = pickle.load(f)\n",
    "with open(token_dict_file, \"rb\") as f:\n",
    "    token_dict = pickle.load(f)\n",
    "\n",
    "# This function simulates tokenization for one cell:\n",
    "# It normalizes each gene's expression by its nonzero median,\n",
    "# then ranks the genes by the normalized expression (highest first),\n",
    "# and finally maps the gene IDs to token IDs using token_dict.\n",
    "def tokenize_cell(expr_vector, ensembl_ids):\n",
    "    gene_norms = []\n",
    "    for idx, expr in enumerate(expr_vector):\n",
    "        # Only consider genes with nonzero expression\n",
    "        if expr != 0:\n",
    "            gene_id = ensembl_ids[idx]\n",
    "            # Use the median if available and > 0; skip otherwise\n",
    "            if gene_id in median_dict and median_dict[gene_id] > 0:\n",
    "                norm_val = expr / median_dict[gene_id]\n",
    "                gene_norms.append((gene_id, norm_val))\n",
    "    # Sort genes by normalized value in descending order\n",
    "    gene_norms_sorted = sorted(gene_norms, key=lambda x: -x[1])\n",
    "    token_seq = []\n",
    "    for gene_id, _ in gene_norms_sorted:\n",
    "        # Only add tokens for genes that exist in the token dictionary.\n",
    "        # (If a gene is missing, it will be skipped.)\n",
    "        if gene_id in token_dict:\n",
    "            token_seq.append(token_dict[gene_id])\n",
    "    return token_seq\n",
    "\n",
    "# Open the loom file and compute tokenized sequence lengths for a subset of cells.\n",
    "token_lengths = []\n",
    "\n",
    "with loompy.connect(loom_file_path) as ds:\n",
    "    ensembl_ids = ds.ra[\"ensembl_id\"]  # Array of gene IDs (rows)\n",
    "    num_genes, num_cells = ds.shape\n",
    "    print(f\"Loom file shape (genes x cells): {ds.shape}\")\n",
    "    \n",
    "    # Process a subset of cells (e.g., first 100 cells)\n",
    "    num_cells_to_process = min(num_cells, 100)\n",
    "    \n",
    "    for cell_idx in range(num_cells_to_process):\n",
    "        # Get the expression vector for the cell (all genes)\n",
    "        expr_vector = ds[:, cell_idx].astype(np.float32)\n",
    "        # Tokenize the cell's gene expression\n",
    "        token_seq = tokenize_cell(expr_vector, ensembl_ids)\n",
    "        token_lengths.append(len(token_seq))\n",
    "    \n",
    "# Print statistics about the tokenized sequence lengths\n",
    "token_lengths = np.array(token_lengths)\n",
    "print(f\"Processed {num_cells_to_process} cells.\")\n",
    "print(\"Token sequence lengths (number of tokens per cell):\")\n",
    "print(token_lengths)\n",
    "print(f\"Average token sequence length: {np.mean(token_lengths):.1f}\")\n",
    "print(f\"Median token sequence length: {np.median(token_lengths):.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenization using TranscriptomeTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geneformer import TranscriptomeTokenizer\n",
    "\n",
    "tk = TranscriptomeTokenizer(custom_attr_name_dict=None, nproc=2, chunk_size=512, model_input_size=4096, \n",
    "                            special_token=True, collapse_gene_ids=True, gene_median_file='/home/logs/jtorresb/Geneformer/yeast/yeast_data/output/yeast_median_dict.pkl',\n",
    "                            token_dictionary_file='/home/logs/jtorresb/Geneformer/yeast/yeast_data/output/yeast_token_dict.pkl', gene_mapping_file=None)\n",
    "# special_token = False was giving problems with \"ensemble_ids_collapse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk.tokenize_data(data_directory=\"/home/logs/jtorresb/Geneformer/yeast/yeast_data/output\",\n",
    "                 output_directory=\"/home/logs/jtorresb/Geneformer/yeast/yeast_data/output\",\n",
    "                 output_prefix=\"yeast_master_matrix_sgd\", \n",
    "                 file_format=\"loom\",\n",
    "                 use_generator=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking .dataset File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           input_ids  length\n",
      "0  [2, 2821, 1289, 612, 3670, 87, 2976, 4704, 590...    4096\n",
      "1  [2, 4580, 6708, 5639, 2013, 6383, 4025, 2786, ...    4096\n",
      "2  [2, 6708, 4580, 2132, 2013, 4462, 5639, 2786, ...    4096\n",
      "3  [2, 6708, 4580, 3453, 2013, 2487, 5639, 2786, ...    4096\n",
      "4  [2, 4580, 6708, 1289, 2786, 2487, 5300, 2013, ...    4096\n",
      "5  [2, 4580, 1289, 2821, 4462, 5901, 1031, 5323, ...    4096\n",
      "6  [2, 4580, 1289, 2821, 4462, 1031, 5901, 5323, ...    4096\n",
      "7  [2, 4580, 1289, 2132, 2821, 5901, 5323, 1031, ...    4096\n",
      "8  [2, 4580, 1289, 2821, 2132, 5323, 1031, 5901, ...    4096\n",
      "9  [2, 4580, 1289, 2132, 2821, 1031, 5323, 5941, ...    4096\n",
      "Total rows: 1550\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Replace the path with the location of your .dataset file\n",
    "dataset_path = \"/home/logs/jtorresb/Geneformer/yeast/yeast_data/output/yeast_master_matrix_sgd.dataset\"\n",
    "data = load_from_disk(dataset_path)\n",
    "\n",
    "# Convert the dataset to a pandas DataFrame\n",
    "df = data.to_pandas()\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head(10))\n",
    "\n",
    "\n",
    "# Filter rows where the length is not 4096\n",
    "not_4096 = df[df[\"length\"] != 4096]\n",
    "print(f\"Total rows: {len(not_4096)}\") # All rows should have length 4096 (interesting to comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           input_ids  length\n",
      "0  [6751, 11578, 5918, 4500, 5707, 16979, 6705, 1...     886\n",
      "1  [3911, 14241, 7375, 6385, 7020, 15538, 1580, 1...      88\n",
      "2  [20154, 9034, 3623, 10248, 6560, 10532, 6171, ...      17\n",
      "3  [20515, 16829, 9401, 10221, 17269, 5205, 17295...      25\n",
      "4  [2183, 5592, 8163, 5074, 14116, 5972, 3584, 19...    1330\n",
      "5  [4260, 9049, 3854, 5262, 9843, 17033, 17084, 1...     543\n",
      "6  [17454, 482, 8115, 20854, 8675, 6297, 20131, 1...      88\n",
      "7  [10158, 11924, 24647, 861, 4733, 3471, 4091, 1...    1021\n",
      "8  [5332, 20154, 6380, 2326, 10219, 3882, 13237, ...      55\n",
      "9  [10390, 16322, 15588, 8874, 5716, 1667, 2565, ...      70\n"
     ]
    }
   ],
   "source": [
    "# Replace the path with the location of your .dataset file\n",
    "dataset_path = \"/home/logs/jtorresb/Geneformer/Genecorpus/example_input_files/gene_classification/dosage_sensitive_tfs/gc-30M_sample50k.dataset\"\n",
    "data = load_from_disk(dataset_path)\n",
    "\n",
    "# Convert the dataset to a pandas DataFrame\n",
    "df = data.to_pandas()\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geneformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
