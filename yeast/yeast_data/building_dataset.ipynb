{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulding Geneformer-Like Dataset From Yeast Master Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming Data and Computing Medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"/home/logs/jtorresb/yeastformer/yeast/yeast_data/output/yeast_master_matrix_unnormalized.csv\"\n",
    "df = pd.read_csv(file_path, sep='\\t', index_col=0)\n",
    "\n",
    "epsilon = 1e-6\n",
    "global_min = df.min().min()  # Find the most negative value in the entire dataset to avoid biasing medians afterwards\n",
    "shifted_df = df - global_min + epsilon  # Shift everything up\n",
    "\n",
    "# Save the processed data\n",
    "shifted_df.to_csv(\"/home/logs/jtorresb/yeastformer/yeast/yeast_data/output/yeast_master_matrix_shifted.csv\", sep='\\t', float_format=\"%.6f\")\n",
    "\n",
    "print(\"Negative values handled properly.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized = shifted_df.div(shifted_df.sum(axis=0), axis=1) * 10_000 # Pizza slicing: divide each value by n_counts, the sum of the column to get proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the median for each gene across all columns, skipping NaNs\n",
    "non_nan_medians = df_normalized.median(axis=1, skipna=True)\n",
    "\n",
    "# Create the dictionary with YORFs as keys and their medians as values\n",
    "median_dict = non_nan_medians.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"/home/logs/jtorresb/yeastformer/yeast/yeast_data/output/updated_yeast_median_dict.pkl\", \"wb\") as f:\n",
    "    pickle.dump(median_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building .loom file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Replacing NaNs by 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"/home/logs/jtorresb/Geneformer/yeast/yeast_data/output/unnormalized_yeast_master_matrix_sgd_copy.csv\"\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(file_path, sep='\\t', index_col=0) # Important to keep sep='\\t', since that's how it was saved \n",
    "\n",
    "# print(df.shape)\n",
    "\n",
    "# Replace NaNs with 0\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Save the cleaned CSV\n",
    "df.to_csv(file_path, sep='\\t')\n",
    "\n",
    "print(\"NaNs replaced with 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Translating to .loom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import loompy\n",
    "\n",
    "input_file_path = \"/home/logs/jtorresb/Geneformer/yeast/yeast_data/output/unnormalized_yeast_master_matrix_sgd_copy.csv\"\n",
    "output_file_path = \"/home/logs/jtorresb/Geneformer/yeast/yeast_data/output/unnormalized_yeast_master_matrix_sgd.loom\"\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(input_file_path, sep='\\t', index_col=0) # Important to keep sep='\\t', since that's how it was saved \n",
    "\n",
    "# Compute total read counts for each experiment (equivalent to cell in geneformer)\n",
    "n_counts = df.sum(axis=0).astype(np.float32)  # Sum across genes for each column \n",
    "\n",
    "# Prepare row attributes (Gene IDs â†’ Ensembl IDs assumed to be index)\n",
    "row_attrs = {\"ensembl_id\": df.index.tolist()}  # Ensure index has Ensembl IDs\n",
    "\n",
    "# Prepare column attributes (Cells & their total read counts)\n",
    "col_attrs = {\n",
    "    # \"exp_name\": df.columns.tolist(),  # Experiment names\n",
    "    \"n_counts\": n_counts.values,   # Total counts per column (experiment)\n",
    "}\n",
    "\n",
    "# Convert DataFrame to Loom format & save\n",
    "loompy.create(output_file_path, df.values.astype(np.float32), row_attrs, col_attrs)\n",
    "\n",
    "print(f\"Loom file saved as: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)  # Should print something like 1.26.4\n",
    "\n",
    "# Had to downgrade numpy < 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Verifying .loom file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loompy\n",
    "\n",
    "input_loom_file_path = \"/home/logs/jtorresb/Geneformer/yeast/yeast_data/output/unnormalized_yeast_master_matrix_sgd.loom\"\n",
    "\n",
    "with loompy.connect(input_loom_file_path) as ds:\n",
    "    # Print general metadata\n",
    "    print(\"Row attributes:\", ds.ra.keys())  # Should contain 'ensembl_id'\n",
    "    print(\"Column attributes:\", ds.ca.keys())  # Should contain 'n_counts'\n",
    "    print(\"Data shape (genes x exp columns):\", ds.shape)\n",
    "\n",
    "    # Print first 5 genes (rows) and their attributes\n",
    "    print(\"\\nFirst 5 Row Attributes:\")\n",
    "    for key in ds.ra.keys():\n",
    "        print(f\"{key}: {ds.ra[key][:5]}\")  # Print first 5 values of each row attribute\n",
    "\n",
    "    # print(f\"exp_name: {ds.ca['exp_name'][:1]}\")\n",
    "    print(f\"n_counts: {ds.ca['n_counts'][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokens Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Load example to see the intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Path to the token dictionary file\n",
    "token_dict_file = \"/home/logs/jtorresb/yeastformer/geneformer/token_dictionary_gc95M.pkl\"\n",
    "\n",
    "# Function to inspect the token dictionary\n",
    "def inspect_token_dictionary(file_path, num_samples=10):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        token_dict = pickle.load(f)\n",
    "    \n",
    "    print(f\"Token dictionary type: {type(token_dict)}\")\n",
    "    print(f\"Total tokens: {len(token_dict)}\")\n",
    "    print(\"First 10 token entries:\")\n",
    "    sample_items = list(token_dict.items())[:num_samples]\n",
    "    for key, value in sample_items:\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "# Run the inspection\n",
    "inspect_token_dictionary(token_dict_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Generating Token Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# File paths\n",
    "csv_file = \"/home/logs/jtorresb/Geneformer/yeast/yeast_data/output/original_yeast_master_matrix_sgd.csv\"\n",
    "output_pkl = \"/home/logs/jtorresb/Geneformer/yeast/yeast_data/output/yeast_token_dict.pkl\"\n",
    "\n",
    "# Load CSV (Ensure YORFs are the index)\n",
    "df = pd.read_csv(csv_file, sep='\\t', index_col=0)\n",
    "\n",
    "# Extract yeast ORFs (YORFs) from index\n",
    "yorfs = df.index.tolist()\n",
    "\n",
    "# Optional: Sort alphabetically for consistency\n",
    "yorfs.sort()\n",
    "\n",
    "# Initialize token dictionary with special tokens\n",
    "token_dict = {\n",
    "    \"<pad>\": 0,\n",
    "    \"<mask>\": 1,\n",
    "    \"<cls>\": 2,\n",
    "    \"<eos>\": 3,\n",
    "}\n",
    "\n",
    "# Assign unique token IDs starting from 4\n",
    "for i, gene_id in enumerate(yorfs, start=4):\n",
    "    token_dict[gene_id] = i\n",
    "\n",
    "# Save dictionary as a pickle file\n",
    "with open(output_pkl, \"wb\") as f:\n",
    "    pickle.dump(token_dict, f)\n",
    "\n",
    "print(f\"Token dictionary saved as: {output_pkl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Medians Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Inspecting example first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Path to the median dictionary file\n",
    "median_dict_file = \"/home/logs/jtorresb/Geneformer/geneformer/gene_median_dictionary_gc95M.pkl\"\n",
    "\n",
    "# Function to inspect the median dictionary\n",
    "def inspect_median_dictionary(file_path, num_samples=10):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        median_dict = pickle.load(f)\n",
    "    \n",
    "    print(f\"Median dictionary type: {type(median_dict)}\")\n",
    "    print(f\"Total genes in dictionary: {len(median_dict)}\")\n",
    "    print(\"First 10 median entries:\")\n",
    "    sample_items = list(median_dict.items())[:num_samples]\n",
    "    for key, value in sample_items:\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "# Run the inspection\n",
    "inspect_median_dictionary(median_dict_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# File paths\n",
    "csv_file = \"/home/logs/jtorresb/Geneformer/yeast/yeast_data/output/original_yeast_master_matrix_sgd_copy.csv\" # Copy already replaced NaNs by 0s\n",
    "output_pkl = \"/home/logs/jtorresb/Geneformer/yeast/yeast_data/output/yeast_median_dict.pkl\"\n",
    "\n",
    "# Load CSV (genes as index, experiments as columns)\n",
    "df = pd.read_csv(csv_file, sep='\\t', index_col=0)\n",
    "\n",
    "# Compute nonzero medians for each gene\n",
    "median_dict = {}\n",
    "for gene in df.index:\n",
    "    nonzero_values = df.loc[gene][df.loc[gene] != 0]  # Ignore zeros\n",
    "    if not nonzero_values.empty:\n",
    "        median_dict[gene] = np.median(nonzero_values)  # Compute median\n",
    "    else:\n",
    "        median_dict[gene] = 0  # If all values are zero, set median to 0\n",
    "\n",
    "# Save dictionary as a pickle file\n",
    "with open(output_pkl, \"wb\") as f:\n",
    "    pickle.dump(median_dict, f)\n",
    "\n",
    "print(f\"Median dictionary saved as: {output_pkl}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing Loom File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quick model_input_size ChecK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loompy\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# File paths (update these as needed)\n",
    "loom_file_path = \"/home/logs/jtorresb/Geneformer/yeast/yeast_data/output/yeast_master_matrix_sgd.loom\"\n",
    "median_dict_file = \"/home/logs/jtorresb/Geneformer/yeast/yeast_data/output/yeast_median_dict.pkl\"\n",
    "token_dict_file = \"/home/logs/jtorresb/Geneformer/yeast/yeast_data/output/yeast_token_dict.pkl\"\n",
    "\n",
    "# Load the median dictionary and token dictionary\n",
    "with open(median_dict_file, \"rb\") as f:\n",
    "    median_dict = pickle.load(f)\n",
    "with open(token_dict_file, \"rb\") as f:\n",
    "    token_dict = pickle.load(f)\n",
    "\n",
    "# This function simulates tokenization for one cell:\n",
    "# It normalizes each gene's expression by its nonzero median,\n",
    "# then ranks the genes by the normalized expression (highest first),\n",
    "# and finally maps the gene IDs to token IDs using token_dict.\n",
    "def tokenize_cell(expr_vector, ensembl_ids):\n",
    "    gene_norms = []\n",
    "    for idx, expr in enumerate(expr_vector):\n",
    "        # Only consider genes with nonzero expression\n",
    "        if expr != 0:\n",
    "            gene_id = ensembl_ids[idx]\n",
    "            # Use the median if available and > 0; skip otherwise\n",
    "            if gene_id in median_dict and median_dict[gene_id] > 0:\n",
    "                norm_val = expr / median_dict[gene_id]\n",
    "                gene_norms.append((gene_id, norm_val))\n",
    "    # Sort genes by normalized value in descending order\n",
    "    gene_norms_sorted = sorted(gene_norms, key=lambda x: -x[1])\n",
    "    token_seq = []\n",
    "    for gene_id, _ in gene_norms_sorted:\n",
    "        # Only add tokens for genes that exist in the token dictionary.\n",
    "        # (If a gene is missing, it will be skipped.)\n",
    "        if gene_id in token_dict:\n",
    "            token_seq.append(token_dict[gene_id])\n",
    "    return token_seq\n",
    "\n",
    "# Open the loom file and compute tokenized sequence lengths for a subset of cells.\n",
    "token_lengths = []\n",
    "\n",
    "with loompy.connect(loom_file_path) as ds:\n",
    "    ensembl_ids = ds.ra[\"ensembl_id\"]  # Array of gene IDs (rows)\n",
    "    num_genes, num_cells = ds.shape\n",
    "    print(f\"Loom file shape (genes x cells): {ds.shape}\")\n",
    "    \n",
    "    # Process a subset of cells (e.g., first 100 cells)\n",
    "    num_cells_to_process = min(num_cells, 100)\n",
    "    \n",
    "    for cell_idx in range(num_cells_to_process):\n",
    "        # Get the expression vector for the cell (all genes)\n",
    "        expr_vector = ds[:, cell_idx].astype(np.float32)\n",
    "        # Tokenize the cell's gene expression\n",
    "        token_seq = tokenize_cell(expr_vector, ensembl_ids)\n",
    "        token_lengths.append(len(token_seq))\n",
    "    \n",
    "# Print statistics about the tokenized sequence lengths\n",
    "token_lengths = np.array(token_lengths)\n",
    "print(f\"Processed {num_cells_to_process} cells.\")\n",
    "print(\"Token sequence lengths (number of tokens per cell):\")\n",
    "print(token_lengths)\n",
    "print(f\"Average token sequence length: {np.mean(token_lengths):.1f}\")\n",
    "print(f\"Median token sequence length: {np.median(token_lengths):.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenization using TranscriptomeTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geneformer import TranscriptomeTokenizer\n",
    "\n",
    "tk = TranscriptomeTokenizer(custom_attr_name_dict=None, nproc=2, chunk_size=512, model_input_size=512, \n",
    "                            special_token=False, collapse_gene_ids=True, gene_median_file='/home/logs/jtorresb/Geneformer/yeast/yeast_data/output/yeast_median_dict.pkl',\n",
    "                            token_dictionary_file='/home/logs/jtorresb/Geneformer/yeast/yeast_data/output/yeast_token_dict.pkl', gene_mapping_file=None)\n",
    "# special_token = False was giving problems with \"ensemble_ids_collapse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk.tokenize_data(data_directory=\"/home/logs/jtorresb/Geneformer/yeast/yeast_data/output/unnormalized\",\n",
    "                 output_directory=\"/home/logs/jtorresb/Geneformer/yeast/yeast_data/output\",\n",
    "                 output_prefix=\"unnormalized_yeast_master_matrix_sgd\", \n",
    "                 file_format=\"loom\",\n",
    "                 use_generator=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking .dataset File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Replace the path with the location of your .dataset file\n",
    "dataset_path = \"/home/logs/jtorresb/Geneformer/yeast/yeast_data/output/unnormalized_yeast_master_matrix_sgd.dataset\"\n",
    "data = load_from_disk(dataset_path)\n",
    "\n",
    "# Convert the dataset to a pandas DataFrame\n",
    "df = data.to_pandas()\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the path with the location of your .dataset file\n",
    "dataset_path = \"/home/logs/jtorresb/Geneformer/Genecorpus/example_input_files/gene_classification/dosage_sensitive_tfs/gc-30M_sample50k.dataset\"\n",
    "data = load_from_disk(dataset_path)\n",
    "\n",
    "# Convert the dataset to a pandas DataFrame\n",
    "df = data.to_pandas()\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Example Lengths File for Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Define dataset path\n",
    "dataset_path = \"/home/logs/jtorresb/Geneformer/yeast/yeast_data/output/yeast_master_matrix_sgd.dataset\"\n",
    "output_pickle_path = \"/home/logs/jtorresb/Geneformer/yeast/yeast_data/output/yeast_lengths.pkl\"\n",
    "\n",
    "# Load dataset\n",
    "data = load_from_disk(dataset_path)\n",
    "\n",
    "# Compute example lengths\n",
    "example_lengths = [data[i][\"length\"] for i in range(len(data))]\n",
    "\n",
    "# Save to pickle file\n",
    "with open(output_pickle_path, \"wb\") as f:\n",
    "    pickle.dump(example_lengths, f)\n",
    "\n",
    "print(f\"Example lengths saved to {output_pickle_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Path to your generated pickle file\n",
    "lengths_file = \"/home/logs/jtorresb/Geneformer/yeast/yeast_data/output/yeast_token_dict.pkl\"\n",
    "\n",
    "# Load the list from the pickle file\n",
    "with open(lengths_file, \"rb\") as f:\n",
    "    example_lengths = pickle.load(f)\n",
    "\n",
    "# Print the first 10 values\n",
    "print(example_lengths)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geneformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
