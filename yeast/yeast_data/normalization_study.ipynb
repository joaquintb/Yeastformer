{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Token Match Percentage: 3.04%\n",
      "Average Jaccard Similarity: 35.83%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Paths to datasets\n",
    "normalized_path = \"/home/logs/jtorresb/Geneformer/yeast/yeast_data/output/yeast_master_matrix_sgd.dataset\"\n",
    "non_normalized_path = \"/home/logs/jtorresb/Geneformer/yeast/yeast_data/output/unnormalized_yeast_master_matrix_sgd.dataset\"\n",
    "\n",
    "# Load datasets\n",
    "normalized_data = load_from_disk(normalized_path)\n",
    "non_normalized_data = load_from_disk(non_normalized_path)\n",
    "\n",
    "# Ensure same number of rows\n",
    "assert len(normalized_data) == len(non_normalized_data), \"Datasets have different lengths!\"\n",
    "\n",
    "# Compare tokenized sequences\n",
    "similarity_scores = []  # Store similarity percentage per row\n",
    "jaccard_scores = []     # Store Jaccard similarity per row\n",
    "\n",
    "for i in range(len(normalized_data)):\n",
    "    tokens_norm = normalized_data[i][\"input_ids\"]\n",
    "    tokens_non_norm = non_normalized_data[i][\"input_ids\"]\n",
    "    \n",
    "    # Ensure same length\n",
    "    min_len = min(len(tokens_norm), len(tokens_non_norm))\n",
    "    tokens_norm = tokens_norm[:min_len]\n",
    "    tokens_non_norm = tokens_non_norm[:min_len]\n",
    "    \n",
    "    # Compute token matching percentage\n",
    "    match_percentage = sum(np.array(tokens_norm) == np.array(tokens_non_norm)) / min_len\n",
    "    similarity_scores.append(match_percentage)\n",
    "\n",
    "    # Compute Jaccard similarity (set-based comparison)\n",
    "    jaccard = len(set(tokens_norm) & set(tokens_non_norm)) / len(set(tokens_norm) | set(tokens_non_norm))\n",
    "    jaccard_scores.append(jaccard)\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"Average Token Match Percentage: {np.mean(similarity_scores) * 100:.2f}%\")\n",
    "print(f\"Average Jaccard Similarity: {np.mean(jaccard_scores) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ First 10 Input IDs (Normalized): [2821, 1289, 612, 3670, 87, 2976, 4704, 5901, 2058, 3236]\n",
      "ðŸ”¹ First 10 Input IDs (Non-Normalized): [2132, 1408, 5326, 4025, 1131, 612, 5323, 3419, 6383, 2058]\n",
      "\n",
      "ðŸ“Œ First few rows of Normalized Dataset:\n",
      "                                            input_ids  length\n",
      "0  [2821, 1289, 612, 3670, 87, 2976, 4704, 5901, ...     512\n",
      "1  [4580, 6708, 5639, 2013, 6383, 4025, 2786, 103...     512\n",
      "2  [6708, 4580, 2132, 2013, 4462, 5639, 2786, 590...     512\n",
      "3  [6708, 4580, 3453, 2013, 2487, 5639, 2786, 446...     512\n",
      "4  [4580, 6708, 1289, 2786, 2487, 5300, 2013, 154...     512\n",
      "\n",
      "ðŸ“Œ First few rows of Non-Normalized Dataset:\n",
      "                                            input_ids  length\n",
      "0  [2132, 1408, 5326, 4025, 1131, 612, 5323, 3419...     512\n",
      "1  [2132, 1408, 5326, 4025, 1131, 612, 5323, 3419...     512\n",
      "2  [2132, 5326, 1408, 4025, 1131, 612, 5323, 3419...     512\n",
      "3  [2132, 1408, 5326, 4025, 1131, 612, 5323, 3419...     512\n",
      "4  [2132, 1408, 5326, 4025, 1131, 5323, 3419, 612...     512\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Paths to datasets\n",
    "normalized_path = \"/home/logs/jtorresb/Geneformer/yeast/yeast_data/output/yeast_master_matrix_sgd.dataset\"\n",
    "non_normalized_path = \"/home/logs/jtorresb/Geneformer/yeast/yeast_data/output/unnormalized_yeast_master_matrix_sgd.dataset\"\n",
    "\n",
    "# Load datasets\n",
    "normalized_data = load_from_disk(normalized_path)\n",
    "non_normalized_data = load_from_disk(non_normalized_path)\n",
    "\n",
    "# Convert to DataFrames\n",
    "df_normalized = pd.DataFrame(normalized_data)\n",
    "df_non_normalized = pd.DataFrame(non_normalized_data)\n",
    "\n",
    "# Extract first row's tokens as a list\n",
    "first_row_norm = df_normalized.iloc[0][\"input_ids\"][:10]  # First 10 tokens\n",
    "first_row_non_norm = df_non_normalized.iloc[0][\"input_ids\"][:10]  # First 10 tokens\n",
    "\n",
    "# Print results\n",
    "print(\"ðŸ”¹ First 10 Input IDs (Normalized):\", first_row_norm)\n",
    "print(\"ðŸ”¹ First 10 Input IDs (Non-Normalized):\", first_row_non_norm)\n",
    "\n",
    "# (Optional) Display first few rows of DataFrames for inspection\n",
    "print(\"\\nðŸ“Œ First few rows of Normalized Dataset:\\n\", df_normalized.head())\n",
    "print(\"\\nðŸ“Œ First few rows of Non-Normalized Dataset:\\n\", df_non_normalized.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geneformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
